DATA:
  # data_root: root folder of dataset (within SAN folder)
  data_root: dataset/ILSVRC2012
  # dataset_init: defines the type of source data to initialize dataset:
  # - image_folder: initializes using torchvision.datasets.ImageFolder. Expects folders 'train', 'val', and possibly 'test'
  # - preprocessed_paths: initialized from custom object file (see torch.save) with images paths and classes. See util.dataset.PathsFileDataset. 
  # Expects files named 'train_init.pt', 'val_init.pt' and possibly 'test_init.pt'.
  # - byte_imgs: initialized from custom object file (see torch.save) containing io.BytesIO representing
  # images. Expects files named 'train_imgs.pt', 'val_imgs.pt' and possibly 'test_imgs.pt'.
  # Transforms for train set (grayscale and random augmentations) will be performed at runtime.
  # Transforms performed on val set are resize to 256x256 followed by center crop (224x224).
  dataset_init: image_folder
  # data_name: seemingly unused
  data_name: imagenet
  # classes: number of classes
  classes: 1000
  # channels: number of channels for input dataset (3 for RGB and 1 for Grayscale).
  channels: 3
  # mean: dataset mean for each input channel, defaults to imagenet values if not defined.
  mean: [0.485, 0.456, 0.406]
  # std: dataset standard deviation for each input channel, defaults to imagenet values if not defined.
  std: [0.229, 0.224, 0.225]


TRAIN:
  # arch: architecture, either 'san', 'resnet' or 'hybrid'.
  # If using hybrid arch, both 'kernels' and 'widths' config params must be set.
  # Length of 'layer' param must be equal to the sum of 'kernels' and 'widths' params.
  arch: san
  # sa_type: san architecture type. 0 is pairwise, 1 is patchwise. Only used if architecture is san.
  sa_type: 0
  # layers: number of blocks used in each stage of model.
  # For example, resnet is based on 4 stages, each with a varying number of bottleneck blocks.
  # Deault SAN is based on 5 stages.
  layers: [2, 1, 2, 4, 1]
  # kernels: size of kernels. ie 3x3, 7x7, etc. Used in SAN (they don't change in classic resnet).
  kernels: [3, 7, 7, 7, 7]
  # widths: resnet channel depth per stage, not used in SAN architecture
  widths:
  # ignore_label: allows a class index to be ignored
  ignore_label: 2000
  # train_gpu: list of gpus to be used, requires correct distributed configurations (see Distributed section below) to have effect.
  train_gpu: [0, 1, 2, 3, 4, 5, 6, 7]
  # workers: data loader workers
  workers: 32 
  # batch_size: batch size for training
  batch_size: 256
  # batch_size_val: batch size for validation during training, memory and speed tradeoff
  batch_size_val: 128
  # base_lr: base learning rate (scheduler changes learning rate over time)
  base_lr: 0.1
  # epochs: number of epochs
  epochs: 100
  # start_epoch: starting epoch number. Affects train sampler and logs. Will be set automatically from checkpoint if 'resume' config is set.
  start_epoch: 0
  # step_epochs: steps at which learning rate is changed. Used only when 'scheduler' config is set to 'step'.
  step_epochs: [30, 60, 90]
  # label_smoothing: Defines label smoothing factor for training. Cross entropy loss tipically considers the ground truth for
  # each class to be either 0 or 1 (each sample corresponds exactly to one class). For various reasons,
  # it can be beneficial to consider a 'smooth' version of that ground truth.
  # If 'label_smoothing' and 'mixup_alpha' configs are undefined, torch.nn.CrossEntropyLoss is used.
  # If 'label_smoothing' is defined, but not 'mixup_alpha', a smooth loss is used with epsilon equal
  # to label_smoothing config.
  # If 'mixup_alpha' is defined, a mixup loss is used (augmentation technique proposed in 'mixup: Beyond Empirical Risk Minimization')
  # with epsilon equal to label_smoothing config (zero if not defined).
  label_smoothing: 0.1
  # mixup_alpha: if defined, performs mixup data augmentation (input images become the mix of two images).
  # This augmentation technique whas proposed in 'mixup: Beyond Empirical Risk Minimization'.
  # The ratio 'lam' that defines how much is taken from each image is defined from a beta
  # distribution with both shape parameters set to mixup_alpha config. Loss is calculated
  # as a weighted sum of each independent loss, ie the loss between the mixup image output and the first
  # image target class, and between the mixup output image and the second image target class,
  # where the ratio is also 'lam'.
  # This mixup loss can also be subject to label smoothing when 'label_smoothing' config is defined.
  mixup_alpha:
  # scheduler: scheduler used. This varies the learning rate over time (epochs). Can be 'step' or 'cosine'.
  scheduler: cosine
  # momentum: momentum used for optimizer
  momentum: 0.9
  # weight_decay: weight decay used for optimizer
  weight_decay: 0.0001
  # manual_seed: If defined, sets the initial seed to be used for all pseudo random tasks. Mainly for replicability.
  manual_seed:
  # print_freq: results will be printed to log every 'print_freq' steps within the epoch.
  print_freq: 10
  # save_freq: checkpoint will be saved every 'save_freq' epochs. Up to 2 checkpoints are kept at any point,
  # in addition to the best epoch run, that will be saved separately.
  save_freq: 1
  # save_path: path that models, log and tensorboard data will be saved to.
  save_path: exp/imagenet/san10_pairwise/model
  # weight: path to initial weight (default: none)
  weight:
  # resume: path to latest checkpoint (default: none)
  resume:
  # evaluate: unused.
  evaluate: True  
# Distributed: Configs for distributed execution. Won't have effect if multiprocessing_distributed is False, and worl_size <=1.
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: True
  world_size: 1
  rank: 0

TEST:
  # use_val_set: if True, use validation set as test set, instead of a separate set,
  # when running test scripts (NOT to confuse with validation during trainig).
  use_val_set: True
  test_gpu: [0]
  test_workers: 10
  batch_size_test: 100
  model_path: exp/imagenet/san10_pairwise/model/model_best.pth