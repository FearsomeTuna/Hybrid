DATA:
  data_root: dataset/MNIST
  data_name: mnist
  classes: 10
  mean: [0.1307]
  std: [0.3081]
  channels: 1 # (3 for RGB and 1 for Gray). Defaults to 3.
  split: 1 # 0 if data is not split (single folder called 'data'), 1 if it's split into train and test folders, 2 if split into train, val and test folders. If 1, train will be split into train and val sets at runtime.
  split_ratio: 0.2 # [train, val, test] list if dataset not split, just val ratio if dataset es split into train and test folders (meaning, ratio used to separate train folder data into train and val). Ignored if already split into train, val and test.

TRAIN:
  arch: resnet
  widths: [64, 128, 256, 512] # resnet channel depth per stage
  depths: [1, 2, 4, 1] # resnet blocks per stage
  ignore_label: 2000
  train_gpu: [0, 1, 2, 3, 4, 5, 6, 7]
  workers: 2  # data loader workers
  batch_size: 32  # batch size for training
  batch_size_val: 16  # batch size for validation during training, memory and speed tradeoff
  base_lr: 0.1
  epochs: 100
  start_epoch: 0
  step_epochs: [30, 60, 90]
  label_smoothing: 0.1
  mixup_alpha:
  scheduler: cosine
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed:
  print_freq: 10
  save_freq: 1
  save_path: exp/mnist/baseline/model
  weight:  # path to initial weight (default: none)
  resume:  # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0

TEST:
  test_gpu: [0]
  test_workers: 10
  batch_size_test: 100
  model_path: exp/mnist/baseline/model/model_best.pth